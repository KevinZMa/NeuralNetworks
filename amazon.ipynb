{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":253591336,"sourceType":"kernelVersion"}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1b81e93f","cell_type":"code","source":"#! pip install -U \"datasets<4.0.0\" torch\n!pip install ipython-autotime tqdm_joblib\n%load_ext autotime\n\n","metadata":{"vscode":{"languageId":"shellscript"},"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T21:18:07.454234Z","iopub.execute_input":"2025-08-01T21:18:07.454458Z","iopub.status.idle":"2025-08-01T21:18:13.181221Z","shell.execute_reply.started":"2025-08-01T21:18:07.454434Z","shell.execute_reply":"2025-08-01T21:18:13.180349Z"}},"outputs":[{"name":"stdout","text":"Collecting ipython-autotime\n  Downloading ipython_autotime-0.3.2-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting tqdm_joblib\n  Downloading tqdm_joblib-0.0.4-py3-none-any.whl.metadata (269 bytes)\nRequirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from ipython-autotime) (7.34.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tqdm_joblib) (4.67.1)\nRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (75.2.0)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (0.7.5)\nRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (5.7.1)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (3.0.51)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (2.19.2)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->ipython-autotime) (4.9.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.13)\nDownloading ipython_autotime-0.3.2-py2.py3-none-any.whl (7.0 kB)\nDownloading tqdm_joblib-0.0.4-py3-none-any.whl (1.7 kB)\nInstalling collected packages: tqdm_joblib, ipython-autotime\nSuccessfully installed ipython-autotime-0.3.2 tqdm_joblib-0.0.4\ntime: 383 µs (started: 2025-08-01 21:18:13 +00:00)\n","output_type":"stream"}],"execution_count":1},{"id":"739706ea","cell_type":"code","source":"from datasets import load_dataset\n\nfrom joblib import Parallel, delayed\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom tqdm import tqdm\nfrom tqdm_joblib import tqdm_joblib\n# from matplotlib import pyplot as plt\n\nimport json\nimport os\nimport time\n\nmps = torch.device(\"mps\")\n\ndataset = load_dataset(\n    \"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Software\", trust_remote_code=True\n)\n# TODO: download to volume\n\"\"\"\nformat:\n{rating: int, title: str, text: str}\n\"\"\"\n\nDO_VALIDATION = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:01:29.878657Z","iopub.execute_input":"2025-07-31T23:01:29.878923Z","iopub.status.idle":"2025-07-31T23:02:49.200677Z","shell.execute_reply.started":"2025-07-31T23:01:29.878897Z","shell.execute_reply":"2025-07-31T23:02:49.199765Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1976fd098ca4a11a37fff8941734db8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Amazon-Reviews-2023.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51052548eb6d4b3d942d6260c4133296"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Software.jsonl:   0%|          | 0.00/1.87G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e4757207b794318ae17b81cb677a515"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating full split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976845f6935342fa867459de78bc6018"}},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"'\\nformat:\\n{rating: int, title: str, text: str}\\n'"},"metadata":{}},{"name":"stdout","text":"time: 1min 19s (started: 2025-07-31 23:01:29 +00:00)\n","output_type":"stream"}],"execution_count":2},{"id":"75aebafd","cell_type":"code","source":"print(f\"MPS available: {torch.backends.mps.is_available()}\")\ncuda_available = torch.cuda.is_available()\nDEVICE = torch.device(\"cuda\" if cuda_available else \"cpu\")\nprint(cuda_available)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:02:49.201545Z","iopub.execute_input":"2025-07-31T23:02:49.201806Z","iopub.status.idle":"2025-07-31T23:02:49.295449Z","shell.execute_reply.started":"2025-07-31T23:02:49.201787Z","shell.execute_reply":"2025-07-31T23:02:49.294727Z"}},"outputs":[{"name":"stdout","text":"MPS available: False\nTrue\ntime: 90 ms (started: 2025-07-31 23:02:49 +00:00)\n","output_type":"stream"}],"execution_count":3},{"id":"c560a895","cell_type":"code","source":"n_samples = len(dataset[\"full\"][\"text\"])  # 480000\nn_samples\n\n# linear to size of dataset\n# linear to number of params\n# linear to number of batches (not size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:02:49.296193Z","iopub.execute_input":"2025-07-31T23:02:49.296455Z","iopub.status.idle":"2025-07-31T23:02:54.768178Z","shell.execute_reply.started":"2025-07-31T23:02:49.296430Z","shell.execute_reply":"2025-07-31T23:02:54.767416Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"4880181"},"metadata":{}},{"name":"stdout","text":"time: 5.47 s (started: 2025-07-31 23:02:49 +00:00)\n","output_type":"stream"}],"execution_count":4},{"id":"f4c2136c","cell_type":"code","source":"# n_samples = n_samples // 100\nX = dataset[\"full\"][\"text\"][:n_samples]\n# X_test = dataset[\"full\"][\"text\"][200000:225000]\n\ny = torch.tensor(dataset[\"full\"][\"rating\"][:n_samples]) >= 4\n\ny","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:02:54.770037Z","iopub.execute_input":"2025-07-31T23:02:54.770259Z","iopub.status.idle":"2025-07-31T23:03:02.881023Z","shell.execute_reply.started":"2025-07-31T23:02:54.770243Z","shell.execute_reply":"2025-07-31T23:03:02.880166Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"tensor([False,  True,  True,  ...,  True,  True,  True])"},"metadata":{}},{"name":"stdout","text":"time: 8.11 s (started: 2025-07-31 23:02:54 +00:00)\n","output_type":"stream"}],"execution_count":5},{"id":"1779cabe-69b2-4be1-ab6e-34bbabbad93d","cell_type":"code","source":"if DO_VALIDATION:\n    test_samples = 10000\n\n    test_dataset = load_dataset(\n        \"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Industrial_and_Scientific\", trust_remote_code=True\n    )\n    \n    X = dataset[\"full\"][\"text\"][:test_samples]\n    y = torch.tensor(dataset[\"full\"][\"rating\"][:test_samples]) >= 4","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b896dd81","cell_type":"code","source":"bag_of_words = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:03:02.881862Z","iopub.execute_input":"2025-07-31T23:03:02.882164Z","iopub.status.idle":"2025-07-31T23:03:02.886604Z","shell.execute_reply.started":"2025-07-31T23:03:02.882138Z","shell.execute_reply":"2025-07-31T23:03:02.885629Z"}},"outputs":[{"name":"stdout","text":"time: 317 µs (started: 2025-07-31 23:03:02 +00:00)\n","output_type":"stream"}],"execution_count":6},{"id":"b75b7c6b","cell_type":"code","source":"import re\n\n\ndef split_text(text: str) -> list[str]:\n    text = text.lower().strip()\n    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n    words = re.split(r\"\\s+\", text)\n    return words\n\n\ntotal_words = 0\n\n\ndef add_to_bag(text: str) -> None:\n    global total_words\n    words = split_text(text)\n    total_words += len(words)\n    for word in words:\n        bag_of_words[word] = bag_of_words.get(word, 0) + 1\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:03:02.887322Z","iopub.execute_input":"2025-07-31T23:03:02.887607Z","iopub.status.idle":"2025-07-31T23:03:02.901227Z","shell.execute_reply.started":"2025-07-31T23:03:02.887579Z","shell.execute_reply":"2025-07-31T23:03:02.900658Z"}},"outputs":[{"name":"stdout","text":"time: 9.68 ms (started: 2025-07-31 23:03:02 +00:00)\n","output_type":"stream"}],"execution_count":7},{"id":"d077ea04","cell_type":"code","source":"# fmt: off\nignored_words = [\n    # articles\n    \"a\", \"an\", \"the\",\n    # pronouns\n    \"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"their\",\"his\",\"her\",\"its\",\"our\",\"their\",\n    # prepositions\n    \"at\", \"in\", \"on\", \"by\", \"for\", \"with\", \"without\", \"to\", \"from\", \"of\", \"about\", \"under\", \"over\", \"through\", \"between\", \"among\", \"during\", \"before\", \"after\", \"above\", \"below\", \"up\", \"down\", \"out\", \"off\", \"into\", \"onto\", \"upon\", \"within\", \"across\", \"along\", \"around\", \"behind\", \"beside\", \"beyond\", \"inside\", \"outside\", \"toward\", \"towards\", \"underneath\", \"against\", \"beneath\", \"near\", \"next\", \"past\", \"since\", \"until\", \"via\",\n    # conjunctions\n    \"and\", \"or\", \"but\", \"so\", \"yet\", \"nor\", \"as\", \"if\", \"when\", \"while\", \"because\", \"although\", \"though\", \"unless\", \"whereas\", \"however\", \"therefore\", \"moreover\", \"furthermore\", \"nevertheless\", \"meanwhile\",\n    # common verbs\n    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"could\", \"should\", \"can\", \"may\", \"might\", \"must\", \"shall\", \"get\", \"got\", \"go\", \"went\", \"come\", \"came\", \"see\", \"saw\", \"know\", \"knew\", \"think\", \"thought\", \"say\", \"said\", \"tell\", \"told\", \"make\", \"made\", \"take\", \"took\", \"give\", \"gave\", \"find\", \"found\", \"use\", \"used\", \"work\", \"worked\", \"look\", \"looked\", \"seem\", \"seemed\", \"feel\", \"felt\", \"try\", \"tried\", \"leave\", \"left\", \"put\", \"set\", \"keep\", \"kept\", \"let\", \"run\", \"ran\", \"move\", \"moved\", \"live\", \"lived\", \"bring\", \"brought\", \"happen\", \"happened\", \"write\", \"wrote\", \"sit\", \"sat\", \"stand\", \"stood\", \"lose\", \"lost\", \"pay\", \"paid\", \"meet\", \"met\", \"include\", \"included\", \"continue\", \"continued\", \"turn\", \"turned\", \"follow\", \"followed\", \"want\", \"wanted\", \"need\", \"needed\", \"like\", \"liked\", \"help\", \"helped\", \"talk\", \"talked\", \"become\", \"became\", \"show\", \"showed\", \"hear\", \"heard\", \"play\", \"played\", \"run\", \"ran\", \"move\", \"moved\", \"live\", \"lived\", \"believe\", \"believed\", \"hold\", \"held\", \"bring\", \"brought\", \"happen\", \"happened\", \"write\", \"wrote\", \"provide\", \"provided\", \"sit\", \"sat\", \"stand\", \"stood\", \"lose\", \"lost\", \"pay\", \"paid\", \"meet\", \"met\", \"include\", \"included\",\n    # adverbs\n    \"very\", \"really\", \"quite\", \"just\", \"only\", \"also\", \"too\", \"so\", \"more\", \"most\", \"much\", \"many\", \"well\", \"good\", \"better\", \"best\", \"bad\", \"worse\", \"worst\", \"little\", \"less\", \"least\", \"big\", \"bigger\", \"biggest\", \"small\", \"smaller\", \"smallest\", \"long\", \"longer\", \"longest\", \"short\", \"shorter\", \"shortest\", \"high\", \"higher\", \"highest\", \"low\", \"lower\", \"lowest\", \"first\", \"last\", \"next\", \"previous\", \"new\", \"old\", \"young\", \"great\", \"right\", \"wrong\", \"true\", \"false\", \"sure\", \"probably\", \"maybe\", \"perhaps\", \"definitely\", \"certainly\", \"absolutely\", \"completely\", \"totally\", \"exactly\", \"almost\", \"nearly\", \"hardly\", \"barely\", \"quite\", \"rather\", \"pretty\", \"fairly\", \"somewhat\", \"slightly\", \"extremely\", \"incredibly\", \"amazingly\", \"surprisingly\", \"unfortunately\", \"fortunately\", \"obviously\", \"clearly\", \"apparently\", \"generally\", \"usually\", \"normally\", \"typically\", \"often\", \"sometimes\", \"rarely\", \"never\", \"always\", \"already\", \"still\", \"yet\", \"soon\", \"now\", \"then\", \"here\", \"there\", \"where\", \"everywhere\", \"anywhere\", \"somewhere\", \"nowhere\", \"how\", \"why\", \"what\", \"when\", \"who\", \"which\", \"whose\", \"whom\",\n    # numbers and quantities\n    \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"first\", \"second\", \"third\", \"another\", \"other\", \"others\", \"some\", \"any\", \"all\", \"each\", \"every\", \"both\", \"either\", \"neither\", \"none\", \"few\", \"several\", \"many\", \"most\", \"much\", \"little\", \"less\", \"more\", \"enough\", \"plenty\",\n    # misc common words\n    \"thing\", \"things\", \"something\", \"anything\", \"nothing\", \"everything\", \"someone\", \"anyone\", \"everyone\", \"no\", \"yes\", \"ok\", \"okay\", \"please\", \"thanks\", \"thank\", \"welcome\", \"hello\", \"hi\", \"bye\", \"goodbye\", \"sorry\", \"excuse\", \"pardon\", \"way\", \"ways\", \"time\", \"times\", \"day\", \"days\", \"year\", \"years\", \"place\", \"places\", \"people\", \"person\", \"man\", \"woman\", \"child\", \"children\", \"life\", \"world\", \"home\", \"house\", \"work\", \"job\", \"money\", \"business\", \"company\", \"part\", \"parts\", \"number\", \"numbers\", \"group\", \"groups\", \"problem\", \"problems\", \"question\", \"questions\", \"answer\", \"answers\", \"fact\", \"facts\", \"example\", \"examples\", \"case\", \"cases\", \"point\", \"points\", \"idea\", \"ideas\", \"information\", \"data\", \"result\", \"results\", \"change\", \"changes\", \"end\", \"beginning\", \"start\", \"finish\", \"side\", \"sides\", \"hand\", \"hands\", \"eye\", \"eyes\", \"head\", \"face\", \"back\", \"front\", \"top\", \"bottom\", \"left\", \"right\", \"inside\", \"outside\", \"important\", \"different\", \"same\", \"such\", \"even\", \"still\", \"however\", \"though\", \"although\", \"since\", \"while\", \"during\", \"before\", \"after\", \"until\", \"unless\", \"because\", \"if\", \"whether\", \"that\", \"this\", \"these\", \"those\", \"there\", \"here\", \"where\", \"when\", \"how\", \"why\", \"what\", \"who\", \"which\", \"whose\", \"whom\",\n    # common contractions\n    \"ive\", \"im\",\n    # parsing-specific\n    \"\", \"br\"\n]\n# fmt: on\n\nbag_of_words = {}\n\nfor observation in X:\n    add_to_bag(observation)\n\nprint(len(bag_of_words))\n\n\nbag_of_words = {\n    k: v for k, v in bag_of_words.items() if v > 10 and k not in ignored_words\n}\nprint(len(bag_of_words))\nbag_size = 3000\nbag_of_words = dict(\n    sorted(bag_of_words.items(), key=lambda x: x[1], reverse=True)[:bag_size]\n)\n\nprint(len(bag_of_words))\nprint(\"avg words per review:\", total_words / len(X))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:03:02.902160Z","iopub.execute_input":"2025-07-31T23:03:02.902445Z","iopub.status.idle":"2025-07-31T23:04:16.140928Z","shell.execute_reply.started":"2025-07-31T23:03:02.902418Z","shell.execute_reply":"2025-07-31T23:04:16.139889Z"}},"outputs":[{"name":"stdout","text":"681990\n57943\n3000\navg words per review: 26.328981445565237\ntime: 1min 13s (started: 2025-07-31 23:03:03 +00:00)\n","output_type":"stream"}],"execution_count":8},{"id":"46450363","cell_type":"code","source":"def vectorize(text: str, word_to_idx: dict[str, int]) -> torch.Tensor:\n    words = split_text(text)\n\n    word_counts = {}  # don't allocate all indices\n\n    for word in words:\n        idx = word_to_idx.get(word)\n        if idx is not None:\n            word_counts[idx] = word_counts.get(idx, 0) + 1\n\n    if not word_counts:\n        return torch.sparse_coo_tensor(\n            torch.empty(\n                (1, 0),  # 1 dimension, 0 non-zero elements (nnz)\n                dtype=torch.long\n            ),\n            [],\n            (len(word_to_idx),),\n            dtype=torch.float16,\n        )\n\n    indices = torch.tensor([list(word_counts.keys())], dtype=torch.long)\n    values = torch.tensor(list(word_counts.values()), dtype=torch.float16)\n\n    return torch.sparse_coo_tensor(\n        indices, values, (len(word_to_idx),), dtype=torch.float16\n    )\n\n\ndef vectorize_parallel(\n    texts: list[str], word_to_idx: dict[str, int], n_jobs: int = -1\n) -> torch.Tensor:\n    with tqdm_joblib(tqdm(total=len(texts))):\n        # 15s vs. threading: 11.8s on 1% of dataset\n        vectors = Parallel(n_jobs=n_jobs, backend=\"threading\")(\n            delayed(vectorize)(text, word_to_idx) for text in texts\n        )\n    return torch.stack(vectors)#.to(DEVICE)\n\n\ndef preprocess(X: list[str]) -> torch.Tensor:\n    word_to_idx = {word: idx for idx, word in enumerate(bag_of_words.keys())}\n\n    return vectorize_parallel(X, word_to_idx)\n\n# TODO: tqdm_joblib\n# from tqdm_joblib import tqdm_joblib\n# with tqdm_joblib(tqdm(total=len(texts))):\nX_processed = preprocess(X)\n# memory\nprint(X_processed.element_size() * X_processed.numel() / 1024**2, \"MB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:04:16.141884Z","iopub.execute_input":"2025-07-31T23:04:16.142259Z","iopub.status.idle":"2025-07-31T23:23:22.746297Z","shell.execute_reply.started":"2025-07-31T23:04:16.142236Z","shell.execute_reply":"2025-07-31T23:23:22.745644Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/4880181 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4880181 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"553119b7f2a8401a922032997670d0e7"}},"metadata":{}},{"name":"stdout","text":"27924.619674682617 MB\ntime: 19min 6s (started: 2025-07-31 23:04:16 +00:00)\n","output_type":"stream"}],"execution_count":9},{"id":"cd54abd1","cell_type":"code","source":"print(len(bag_of_words))\nprint(len(X_processed))\n\nLAYERS = 124, 42, 14, 6\n# LAYER_1, LAYER_2 = 75, 5  # original\nLAYER_1, LAYER_2, LAYER_3, LAYER_4 = LAYERS\n\nbag_size = len(bag_of_words)\n\nprint(\"num params L1:\", len(bag_of_words) * LAYER_1 + LAYER_1)\nnum_params = (\n    sum(LAYERS[i - 1] * LAYERS[i] + LAYERS[i] for i in range(1, len(LAYERS)))\n    + bag_size * LAYERS[0]\n    + LAYERS[0]\n    + LAYERS[-1]\n    + 1\n)\nprint(\"num params:\", num_params)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:23:22.748472Z","iopub.execute_input":"2025-07-31T23:23:22.748702Z","iopub.status.idle":"2025-07-31T23:23:22.754424Z","shell.execute_reply.started":"2025-07-31T23:23:22.748683Z","shell.execute_reply":"2025-07-31T23:23:22.753609Z"}},"outputs":[{"name":"stdout","text":"3000\n4880181\nnum params L1: 372124\nnum params: 378073\ntime: 999 µs (started: 2025-07-31 23:23:22 +00:00)\n","output_type":"stream"}],"execution_count":10},{"id":"c98b7f7a","cell_type":"code","source":"from torch.nn import init\nimport math\n\nclass SparseLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n        \"\"\"Source copied from torch.nn.Linear @ https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nn/modules/linear.py#L50\"\"\"\n        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(\n            torch.empty((out_features, in_features), **factory_kwargs)\n        )\n        if bias:\n            self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs))\n        else:\n            self.register_parameter(\"bias\", None)\n        self.reset_parameters()\n\n    def forward(self, x):\n        x_sparse = x.to(self.weight.dtype).coalesce()  # ensures unique indices\n        out = torch.sparse.mm(\n            x_sparse,  # (B x in) · (out x in)ᵀ -> (B x out)\n            self.weight.t(),\n        )\n        if self.bias is not None:\n            out = out + self.bias\n        return out\n    \n    def reset_parameters(self) -> None:\n        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n        # https://github.com/pytorch/pytorch/issues/57109\n        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n        if self.bias is not None:\n            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n            init.uniform_(self.bias, -bound, bound)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:23:22.755249Z","iopub.execute_input":"2025-07-31T23:23:22.755497Z","iopub.status.idle":"2025-07-31T23:23:22.773293Z","shell.execute_reply.started":"2025-07-31T23:23:22.755472Z","shell.execute_reply":"2025-07-31T23:23:22.772652Z"}},"outputs":[{"name":"stdout","text":"time: 12.3 ms (started: 2025-07-31 23:23:22 +00:00)\n","output_type":"stream"}],"execution_count":11},{"id":"f7678419-f9f8-4c9d-9219-abd84cd22eca","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ec154c12","cell_type":"code","source":"json.dump(bag_of_words, open(f\"/kaggle/working/bag_of_words_{time.time()}.json\", \"w\"))\n\nCONFIG = {\n    \"PARAMS\": num_params,\n    \"BATCHES\": 1000,\n    \"EPOCHS\": 100,\n    \"SAMPLES\": n_samples,#int(n_samples * 0.8),\n    \"LR\": 0.003,\n    \"BAG_SIZE\": bag_size,\n}\nif False:  # test runs\n    CONFIG = {\n        \"PARAMS\": num_params,\n        \"BATCHES\": 1000,\n        \"EPOCHS\": 10,\n        \"SAMPLES\": int(n_samples * 0.8),\n        \"LR\": 0.003,\n        \"BAG_SIZE\": bag_size,\n    }\n\n\nprint(CONFIG)\n\ndef sparse_collate_fn(batch):\n    inputs, targets = zip(*batch)\n    # Stack sparse tensors along batch dimension\n    batched_input = torch.stack(inputs, dim=0)\n    batched_targets = torch.stack(targets, dim=0)\n    return batched_input, batched_targets\n\n\ndef run_nn():\n    # optimal params = 10% of 80000 = 8000\n    model = nn.Sequential(\n        SparseLinear(len(bag_of_words), LAYER_1),\n        nn.ReLU(),\n        nn.Linear(LAYER_1, LAYER_2),\n        nn.ReLU(),\n        nn.Linear(LAYER_2, LAYER_3),\n        nn.ReLU(),\n        nn.Linear(LAYER_3, LAYER_4),\n        nn.ReLU(),\n        nn.Linear(LAYER_4, 1),\n        # nn.Sigmoid(),  # using logits loss\n    ).to(DEVICE)  # 25 it/s -> 88 it/s\n    model = torch.compile(model)\n    # model = nn.DataParallel(model)  # use both T4s\n\n    # TODO: clear model if instantiating once\n\n    print(model)\n\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"LR\"])\n\n    n_samples = CONFIG[\"SAMPLES\"]\n    # X_train, X_test = X_processed[:n_samples], X_processed[n_samples:]\n    # y_train, y_test = y[:n_samples].float(), y[n_samples:].float()\n\n    X_train = X_processed.half().to(DEVICE)\n    y_train = y.half().to(DEVICE)\n\n    dataset = TensorDataset(X_train, y_train)\n    dataloader = DataLoader(\n        dataset, batch_size=len(X_train) // CONFIG[\"BATCHES\"], shuffle=True,\n        collate_fn=sparse_collate_fn\n\n    )\n    # TODO: sparse tensors?\n\n    # loss_per_batch = []\n\n    start_time = time.time()\n\n    for epoch in range(CONFIG[\"EPOCHS\"]):\n        print(f\"[{time.time()}] Epoch {epoch}\")\n        for X_batch, y_batch in tqdm(dataloader):\n            # already allocated, removing to(mps) shaved 33% of time\n            X_batch, y_batch = X_batch.to(DEVICE, non_blocking=True), y_batch.to(DEVICE, non_blocking=True)\n            optimizer.zero_grad()\n            y_pred = model(X_batch)\n            loss = loss_fn(y_pred.squeeze(-1), y_batch)\n            loss.backward()\n            optimizer.step()\n            # loss_per_batch.append(loss.item())\n\n        tm = time.time()\n        torch.save(model, f\"/kaggle/working/model_epoch{epoch}_{tm}.pth\")\n\n    end_time = time.time()\n    print(f\"Time taken: {end_time - start_time:.4f} seconds\")\n\n    return model\n\n    # with torch.no_grad():\n    #     y_pred_train = model(X_train).squeeze(-1)\n    #     y_pred_test = model(X_test).squeeze(-1)\n\n    # # sigmoid to binary\n    # train_preds = (y_pred_train >= 0.5).int()\n    # test_preds = (y_pred_test >= 0.5).int()\n\n    # num_true_train = (y_train == 1).sum()\n    # num_true_test = (y_test == 1).sum()\n    # train_acc = (train_preds == y_train.int()).float().mean()\n    # test_acc = (test_preds == y_test.int()).float().mean()\n\n    # # plt.xlabel(\"Batch\")\n    # # plt.ylabel(\"Loss\")\n    # # plt.plot(loss_per_batch)\n    # # plt.show()\n    # # print(\"Final loss:\", loss_per_batch[-1])\n\n    # print(\n    #     f\"Train Accuracy: {train_acc:.4f} ({train_preds.sum()}/{len(X_train)}), random guess accuracy: {max(num_true_train, len(X_train) - num_true_train) / len(X_train):.4f}\"\n    # )\n    # print(\n    #     f\"Test Accuracy: {test_acc:.4f} ({test_preds.sum()}/{len(X_test)}), random guess accuracy: {max(num_true_test, len(X_test) - num_true_test) / len(X_test):.4f}\"\n    # )\n\n    # precision = (\n    #     test_preds * y_test\n    # ).sum() / test_preds.sum()  # true pos / predicted pos\n    # recall = (test_preds * y_test).sum() / y_test.sum()  # true pos / actual pos\n    # f1 = 2 * precision * recall / (precision + recall)\n    # print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n\n    # return model\n\n\nm = run_nn()\nprint(CONFIG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T23:23:22.774135Z","iopub.execute_input":"2025-07-31T23:23:22.774369Z","execution_failed":"2025-08-01T00:14:00.619Z"}},"outputs":[{"name":"stdout","text":"{'PARAMS': 378073, 'BATCHES': 1000, 'EPOCHS': 100, 'SAMPLES': 4880181, 'LR': 0.003, 'BAG_SIZE': 3000}\nOptimizedModule(\n  (_orig_mod): Sequential(\n    (0): SparseLinear()\n    (1): ReLU()\n    (2): Linear(in_features=124, out_features=42, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=42, out_features=14, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=14, out_features=6, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=6, out_features=1, bias=True)\n  )\n)\n[1754004207.841067] Epoch 0\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/1001 [00:00<?, ?it/s]\u001b[A\n  0%|          | 1/1001 [00:11<3:18:01, 11.88s/it]\u001b[A\n  0%|          | 2/1001 [00:21<3:00:24, 10.84s/it]\u001b[A\n  0%|          | 3/1001 [00:32<2:54:52, 10.51s/it]\u001b[A\n  0%|          | 4/1001 [00:42<2:52:17, 10.37s/it]\u001b[A\n  0%|          | 5/1001 [00:52<2:51:05, 10.31s/it]\u001b[A\n  1%|          | 6/1001 [01:02<2:50:40, 10.29s/it]\u001b[A\n  1%|          | 7/1001 [01:13<2:50:40, 10.30s/it]\u001b[A\n  1%|          | 8/1001 [01:23<2:50:52, 10.32s/it]\u001b[A\n  1%|          | 9/1001 [01:33<2:51:08, 10.35s/it]\u001b[A\n  1%|          | 10/1001 [01:44<2:51:27, 10.38s/it]\u001b[A\n  1%|          | 11/1001 [01:54<2:51:39, 10.40s/it]\u001b[A\n  1%|          | 12/1001 [02:05<2:51:38, 10.41s/it]\u001b[A\n  1%|▏         | 13/1001 [02:15<2:51:24, 10.41s/it]\u001b[A\n  1%|▏         | 14/1001 [02:25<2:50:53, 10.39s/it]\u001b[A\n  1%|▏         | 15/1001 [02:36<2:50:49, 10.39s/it]\u001b[A\n  2%|▏         | 16/1001 [02:46<2:50:23, 10.38s/it]\u001b[A\n  2%|▏         | 17/1001 [02:57<2:50:14, 10.38s/it]\u001b[A\n  2%|▏         | 18/1001 [03:07<2:50:10, 10.39s/it]\u001b[A\n  2%|▏         | 19/1001 [03:17<2:50:00, 10.39s/it]\u001b[A\n  2%|▏         | 20/1001 [03:28<2:49:53, 10.39s/it]\u001b[A\n  2%|▏         | 21/1001 [03:38<2:49:43, 10.39s/it]\u001b[A\n  2%|▏         | 22/1001 [03:49<2:49:46, 10.41s/it]\u001b[A\n  2%|▏         | 23/1001 [03:59<2:49:33, 10.40s/it]\u001b[A\n  2%|▏         | 24/1001 [04:09<2:49:15, 10.39s/it]\u001b[A\n  2%|▏         | 25/1001 [04:20<2:49:01, 10.39s/it]\u001b[A\n  3%|▎         | 26/1001 [04:30<2:48:56, 10.40s/it]\u001b[A\n  3%|▎         | 27/1001 [04:41<2:48:50, 10.40s/it]\u001b[A\n  3%|▎         | 28/1001 [04:51<2:48:45, 10.41s/it]\u001b[A\n  3%|▎         | 29/1001 [05:01<2:48:36, 10.41s/it]\u001b[A\n  3%|▎         | 30/1001 [05:12<2:48:33, 10.42s/it]\u001b[A\n  3%|▎         | 31/1001 [05:22<2:48:26, 10.42s/it]\u001b[A\n  3%|▎         | 32/1001 [05:33<2:48:17, 10.42s/it]\u001b[A\n  3%|▎         | 33/1001 [05:43<2:48:07, 10.42s/it]\u001b[A\n  3%|▎         | 34/1001 [05:53<2:47:56, 10.42s/it]\u001b[A\n  3%|▎         | 35/1001 [06:04<2:47:46, 10.42s/it]\u001b[A\n  4%|▎         | 36/1001 [06:14<2:47:36, 10.42s/it]\u001b[A\n  4%|▎         | 37/1001 [06:25<2:47:23, 10.42s/it]\u001b[A\n  4%|▍         | 38/1001 [06:35<2:47:15, 10.42s/it]\u001b[A\n  4%|▍         | 39/1001 [06:46<2:47:05, 10.42s/it]\u001b[A\n  4%|▍         | 40/1001 [06:56<2:46:55, 10.42s/it]\u001b[A\n  4%|▍         | 41/1001 [07:06<2:46:46, 10.42s/it]\u001b[A\n  4%|▍         | 42/1001 [07:17<2:46:36, 10.42s/it]\u001b[A\n  4%|▍         | 43/1001 [07:27<2:46:25, 10.42s/it]\u001b[A\n  4%|▍         | 44/1001 [07:38<2:46:15, 10.42s/it]\u001b[A\n  4%|▍         | 45/1001 [07:48<2:46:05, 10.42s/it]\u001b[A\n  5%|▍         | 46/1001 [07:59<2:45:55, 10.42s/it]\u001b[A\n  5%|▍         | 47/1001 [08:09<2:45:45, 10.42s/it]\u001b[A\n  5%|▍         | 48/1001 [08:19<2:45:35, 10.43s/it]\u001b[A\n  0%|          | 0/4880181 [27:49<?, ?it/s].42s/it]\u001b[A\n\n  5%|▍         | 50/1001 [08:41<2:46:54, 10.53s/it]\u001b[A\n  5%|▌         | 51/1001 [08:51<2:46:13, 10.50s/it]\u001b[A\n  5%|▌         | 52/1001 [09:01<2:45:42, 10.48s/it]\u001b[A\n  5%|▌         | 53/1001 [09:12<2:45:19, 10.46s/it]\u001b[A\n  5%|▌         | 54/1001 [09:22<2:44:58, 10.45s/it]\u001b[A\n  5%|▌         | 55/1001 [09:33<2:44:41, 10.45s/it]\u001b[A\n  6%|▌         | 56/1001 [09:43<2:44:23, 10.44s/it]\u001b[A\n  6%|▌         | 57/1001 [09:54<2:44:08, 10.43s/it]\u001b[A\n  6%|▌         | 58/1001 [10:04<2:43:54, 10.43s/it]\u001b[A\n  6%|▌         | 59/1001 [10:15<2:45:22, 10.53s/it]\u001b[A\n  6%|▌         | 60/1001 [10:25<2:44:41, 10.50s/it]\u001b[A\n  6%|▌         | 61/1001 [10:36<2:44:11, 10.48s/it]\u001b[A\n  6%|▌         | 62/1001 [10:46<2:43:45, 10.46s/it]\u001b[A\n  6%|▋         | 63/1001 [10:57<2:43:24, 10.45s/it]\u001b[A\n  6%|▋         | 64/1001 [11:07<2:43:04, 10.44s/it]\u001b[A\n  6%|▋         | 65/1001 [11:17<2:42:46, 10.43s/it]\u001b[A\n  7%|▋         | 66/1001 [11:28<2:42:33, 10.43s/it]\u001b[A\n  7%|▋         | 67/1001 [11:38<2:42:20, 10.43s/it]\u001b[A\n  7%|▋         | 68/1001 [11:49<2:42:09, 10.43s/it]\u001b[A\n  7%|▋         | 69/1001 [11:59<2:43:36, 10.53s/it]\u001b[A\n  7%|▋         | 70/1001 [12:10<2:42:55, 10.50s/it]\u001b[A\n  7%|▋         | 71/1001 [12:20<2:42:24, 10.48s/it]\u001b[A\n  7%|▋         | 72/1001 [12:31<2:41:58, 10.46s/it]\u001b[A\n  7%|▋         | 73/1001 [12:41<2:41:36, 10.45s/it]\u001b[A\n  7%|▋         | 74/1001 [12:52<2:41:19, 10.44s/it]\u001b[A\n  7%|▋         | 75/1001 [13:02<2:41:03, 10.44s/it]\u001b[A\n  8%|▊         | 76/1001 [13:12<2:40:49, 10.43s/it]\u001b[A\n  8%|▊         | 77/1001 [13:23<2:40:35, 10.43s/it]\u001b[A\n  8%|▊         | 78/1001 [13:33<2:40:27, 10.43s/it]\u001b[A\n  8%|▊         | 79/1001 [13:44<2:41:52, 10.53s/it]\u001b[A\n  8%|▊         | 80/1001 [13:54<2:41:13, 10.50s/it]\u001b[A\n  8%|▊         | 81/1001 [14:05<2:40:41, 10.48s/it]\u001b[A\n  8%|▊         | 82/1001 [14:15<2:40:17, 10.46s/it]\u001b[A\n  8%|▊         | 83/1001 [14:26<2:40:05, 10.46s/it]\u001b[A\n  8%|▊         | 84/1001 [14:36<2:39:44, 10.45s/it]\u001b[A\n  8%|▊         | 85/1001 [14:47<2:39:29, 10.45s/it]\u001b[A\n  9%|▊         | 86/1001 [14:57<2:39:12, 10.44s/it]\u001b[A\n  9%|▊         | 87/1001 [15:08<2:40:32, 10.54s/it]\u001b[A\n  9%|▉         | 88/1001 [15:18<2:39:49, 10.50s/it]\u001b[A\n  9%|▉         | 89/1001 [15:29<2:39:18, 10.48s/it]\u001b[A\n  9%|▉         | 90/1001 [15:39<2:38:55, 10.47s/it]\u001b[A\n  9%|▉         | 91/1001 [15:50<2:38:34, 10.46s/it]\u001b[A\n  9%|▉         | 92/1001 [16:00<2:38:16, 10.45s/it]\u001b[A\n  9%|▉         | 93/1001 [16:10<2:37:59, 10.44s/it]\u001b[A\n  9%|▉         | 94/1001 [16:21<2:37:43, 10.43s/it]\u001b[A\n  9%|▉         | 95/1001 [16:31<2:37:28, 10.43s/it]\u001b[A\n 10%|▉         | 96/1001 [16:42<2:37:14, 10.42s/it]\u001b[A\n 10%|▉         | 97/1001 [16:52<2:38:38, 10.53s/it]\u001b[A\n 10%|▉         | 98/1001 [17:03<2:37:58, 10.50s/it]\u001b[A\n 10%|▉         | 99/1001 [17:13<2:37:13, 10.46s/it]\u001b[A\n 10%|▉         | 100/1001 [17:24<2:36:39, 10.43s/it]\u001b[A\n 10%|█         | 101/1001 [17:34<2:36:21, 10.42s/it]\u001b[A\n 10%|█         | 102/1001 [17:44<2:36:10, 10.42s/it]\u001b[A\n 10%|█         | 103/1001 [17:55<2:36:02, 10.43s/it]\u001b[A\n 10%|█         | 104/1001 [18:05<2:35:47, 10.42s/it]\u001b[A\n 10%|█         | 105/1001 [18:16<2:35:38, 10.42s/it]\u001b[A\n 11%|█         | 106/1001 [18:26<2:37:00, 10.53s/it]\u001b[A\n 11%|█         | 107/1001 [18:37<2:36:12, 10.48s/it]\u001b[A\n 11%|█         | 108/1001 [18:47<2:35:36, 10.46s/it]\u001b[A\n 11%|█         | 109/1001 [18:58<2:35:08, 10.44s/it]\u001b[A\n 11%|█         | 110/1001 [19:08<2:34:46, 10.42s/it]\u001b[A\n 11%|█         | 111/1001 [19:18<2:34:28, 10.41s/it]\u001b[A\n 11%|█         | 112/1001 [19:29<2:34:10, 10.41s/it]\u001b[A\n 11%|█▏        | 113/1001 [19:39<2:33:52, 10.40s/it]\u001b[A\n 11%|█▏        | 114/1001 [19:49<2:33:37, 10.39s/it]\u001b[A\n 11%|█▏        | 115/1001 [20:00<2:35:08, 10.51s/it]\u001b[A\n 12%|█▏        | 116/1001 [20:11<2:34:36, 10.48s/it]\u001b[A\n 12%|█▏        | 117/1001 [20:21<2:34:09, 10.46s/it]\u001b[A\n 12%|█▏        | 118/1001 [20:32<2:33:48, 10.45s/it]\u001b[A\n 12%|█▏        | 119/1001 [20:42<2:33:30, 10.44s/it]\u001b[A\n 12%|█▏        | 120/1001 [20:52<2:33:15, 10.44s/it]\u001b[A\n 12%|█▏        | 121/1001 [21:03<2:32:58, 10.43s/it]\u001b[A\n 12%|█▏        | 122/1001 [21:13<2:32:48, 10.43s/it]\u001b[A\n 12%|█▏        | 123/1001 [21:24<2:32:34, 10.43s/it]\u001b[A\n 12%|█▏        | 124/1001 [21:34<2:32:20, 10.42s/it]\u001b[A\n 12%|█▏        | 125/1001 [21:45<2:33:31, 10.51s/it]\u001b[A\n 13%|█▎        | 126/1001 [21:55<2:32:41, 10.47s/it]\u001b[A\n 13%|█▎        | 127/1001 [22:06<2:32:12, 10.45s/it]\u001b[A\n 13%|█▎        | 128/1001 [22:16<2:31:55, 10.44s/it]\u001b[A\n 13%|█▎        | 129/1001 [22:26<2:31:40, 10.44s/it]\u001b[A\n 13%|█▎        | 130/1001 [22:37<2:31:26, 10.43s/it]\u001b[A\n 13%|█▎        | 131/1001 [22:47<2:31:13, 10.43s/it]\u001b[A\n 13%|█▎        | 132/1001 [22:58<2:31:03, 10.43s/it]\u001b[A\n 13%|█▎        | 133/1001 [23:08<2:30:49, 10.43s/it]\u001b[A\n 13%|█▎        | 134/1001 [23:19<2:30:36, 10.42s/it]\u001b[A\n 13%|█▎        | 135/1001 [23:29<2:31:59, 10.53s/it]\u001b[A\n 14%|█▎        | 136/1001 [23:40<2:31:18, 10.50s/it]\u001b[A\n 14%|█▎        | 137/1001 [23:50<2:30:38, 10.46s/it]\u001b[A\n 14%|█▍        | 138/1001 [24:00<2:30:07, 10.44s/it]\u001b[A\n 14%|█▍        | 139/1001 [24:11<2:29:47, 10.43s/it]\u001b[A\n 14%|█▍        | 140/1001 [24:21<2:29:34, 10.42s/it]\u001b[A\n 14%|█▍        | 141/1001 [24:32<2:29:22, 10.42s/it]\u001b[A\n 14%|█▍        | 142/1001 [24:42<2:29:10, 10.42s/it]\u001b[A\n 14%|█▍        | 143/1001 [24:53<2:28:55, 10.41s/it]\u001b[A\n 14%|█▍        | 144/1001 [25:03<2:28:47, 10.42s/it]\u001b[A\n 14%|█▍        | 145/1001 [25:14<2:30:07, 10.52s/it]\u001b[A\n 15%|█▍        | 146/1001 [25:24<2:29:31, 10.49s/it]\u001b[A\n 15%|█▍        | 147/1001 [25:35<2:29:02, 10.47s/it]\u001b[A\n 15%|█▍        | 148/1001 [25:45<2:28:32, 10.45s/it]\u001b[A\n 15%|█▍        | 149/1001 [25:55<2:28:09, 10.43s/it]\u001b[A\n 15%|█▍        | 150/1001 [26:06<2:27:52, 10.43s/it]\u001b[A\n 15%|█▌        | 151/1001 [26:16<2:27:30, 10.41s/it]\u001b[A\n 15%|█▌        | 152/1001 [26:27<2:27:11, 10.40s/it]\u001b[A\n 15%|█▌        | 153/1001 [26:37<2:26:55, 10.40s/it]\u001b[A\n 15%|█▌        | 154/1001 [26:47<2:26:43, 10.39s/it]\u001b[A\n 15%|█▌        | 155/1001 [26:58<2:28:01, 10.50s/it]\u001b[A\n 16%|█▌        | 156/1001 [27:08<2:27:30, 10.47s/it]\u001b[A\n 16%|█▌        | 157/1001 [27:19<2:27:05, 10.46s/it]\u001b[A\n 16%|█▌        | 158/1001 [27:29<2:26:43, 10.44s/it]\u001b[A\n 16%|█▌        | 159/1001 [27:40<2:26:26, 10.44s/it]\u001b[A\n 16%|█▌        | 160/1001 [27:50<2:26:11, 10.43s/it]\u001b[A\n 16%|█▌        | 161/1001 [28:01<2:25:59, 10.43s/it]\u001b[A\n 16%|█▌        | 162/1001 [28:11<2:25:48, 10.43s/it]\u001b[A\n 16%|█▋        | 163/1001 [28:21<2:25:37, 10.43s/it]\u001b[A\n 16%|█▋        | 164/1001 [28:32<2:25:26, 10.43s/it]\u001b[A\n 16%|█▋        | 165/1001 [28:43<2:26:40, 10.53s/it]\u001b[A\n 17%|█▋        | 166/1001 [28:53<2:26:03, 10.50s/it]\u001b[A\n 17%|█▋        | 167/1001 [29:03<2:25:40, 10.48s/it]\u001b[A\n 17%|█▋        | 168/1001 [29:14<2:25:14, 10.46s/it]\u001b[A\n 17%|█▋        | 169/1001 [29:24<2:24:52, 10.45s/it]\u001b[A\n 17%|█▋        | 170/1001 [29:35<2:24:35, 10.44s/it]\u001b[A\n 17%|█▋        | 171/1001 [29:45<2:24:19, 10.43s/it]\u001b[A\n 17%|█▋        | 172/1001 [29:56<2:24:05, 10.43s/it]\u001b[A\n 17%|█▋        | 173/1001 [30:06<2:23:52, 10.43s/it]\u001b[A\n 17%|█▋        | 174/1001 [30:17<2:25:05, 10.53s/it]\u001b[A\n 17%|█▋        | 175/1001 [30:27<2:24:28, 10.49s/it]\u001b[A\n 18%|█▊        | 176/1001 [30:38<2:23:58, 10.47s/it]\u001b[A\n 18%|█▊        | 177/1001 [30:48<2:23:34, 10.45s/it]\u001b[A\n 18%|█▊        | 178/1001 [30:58<2:23:15, 10.44s/it]\u001b[A\n 18%|█▊        | 179/1001 [31:09<2:23:02, 10.44s/it]\u001b[A\n 18%|█▊        | 180/1001 [31:19<2:22:48, 10.44s/it]\u001b[A\n 18%|█▊        | 181/1001 [31:30<2:22:33, 10.43s/it]\u001b[A\n 18%|█▊        | 182/1001 [31:40<2:22:19, 10.43s/it]\u001b[A\n 18%|█▊        | 183/1001 [31:50<2:22:07, 10.43s/it]\u001b[A\n 18%|█▊        | 184/1001 [32:01<2:23:20, 10.53s/it]\u001b[A\n 18%|█▊        | 185/1001 [32:12<2:22:44, 10.50s/it]\u001b[A\n 19%|█▊        | 186/1001 [32:22<2:22:16, 10.47s/it]\u001b[A\n 19%|█▊        | 187/1001 [32:33<2:21:55, 10.46s/it]\u001b[A\n 19%|█▉        | 188/1001 [32:43<2:21:36, 10.45s/it]\u001b[A\n 19%|█▉        | 189/1001 [32:53<2:21:16, 10.44s/it]\u001b[A\n 19%|█▉        | 190/1001 [33:04<2:20:58, 10.43s/it]\u001b[A\n 19%|█▉        | 191/1001 [33:14<2:20:38, 10.42s/it]\u001b[A\n 19%|█▉        | 192/1001 [33:25<2:20:14, 10.40s/it]\u001b[A\n 19%|█▉        | 193/1001 [33:35<2:21:32, 10.51s/it]\u001b[A\n 19%|█▉        | 194/1001 [33:46<2:21:08, 10.49s/it]\u001b[A\n 19%|█▉        | 195/1001 [33:56<2:20:39, 10.47s/it]\u001b[A\n 20%|█▉        | 196/1001 [34:07<2:20:17, 10.46s/it]\u001b[A\n 20%|█▉        | 197/1001 [34:17<2:19:56, 10.44s/it]\u001b[A\n 20%|█▉        | 198/1001 [34:27<2:19:33, 10.43s/it]\u001b[A\n 20%|█▉        | 199/1001 [34:38<2:19:04, 10.40s/it]\u001b[A\n 20%|█▉        | 200/1001 [34:48<2:18:57, 10.41s/it]\u001b[A\n 20%|██        | 201/1001 [34:59<2:18:56, 10.42s/it]\u001b[A\n 20%|██        | 202/1001 [35:09<2:18:52, 10.43s/it]\u001b[A\n 20%|██        | 203/1001 [35:20<2:20:10, 10.54s/it]\u001b[A\n 20%|██        | 204/1001 [35:30<2:19:23, 10.49s/it]\u001b[A\n 20%|██        | 205/1001 [35:41<2:19:02, 10.48s/it]\u001b[A\n 21%|██        | 206/1001 [35:51<2:18:41, 10.47s/it]\u001b[A\n 21%|██        | 207/1001 [36:02<2:18:09, 10.44s/it]\u001b[A\n 21%|██        | 208/1001 [36:12<2:17:46, 10.42s/it]\u001b[A\n 21%|██        | 209/1001 [36:22<2:17:31, 10.42s/it]\u001b[A\n 21%|██        | 210/1001 [36:33<2:17:18, 10.42s/it]\u001b[A\n 21%|██        | 211/1001 [36:43<2:17:12, 10.42s/it]\u001b[A\n 21%|██        | 212/1001 [36:54<2:18:28, 10.53s/it]\u001b[A\n 21%|██▏       | 213/1001 [37:04<2:18:04, 10.51s/it]\u001b[A\n 21%|██▏       | 214/1001 [37:15<2:17:38, 10.49s/it]\u001b[A\n 21%|██▏       | 215/1001 [37:25<2:17:18, 10.48s/it]\u001b[A\n 22%|██▏       | 216/1001 [37:36<2:17:04, 10.48s/it]\u001b[A\n 22%|██▏       | 217/1001 [37:46<2:16:51, 10.47s/it]\u001b[A\n 22%|██▏       | 218/1001 [37:57<2:16:37, 10.47s/it]\u001b[A\n 22%|██▏       | 219/1001 [38:07<2:16:23, 10.46s/it]\u001b[A\n 22%|██▏       | 220/1001 [38:18<2:16:09, 10.46s/it]\u001b[A\n 22%|██▏       | 221/1001 [38:28<2:17:16, 10.56s/it]\u001b[A\n 22%|██▏       | 222/1001 [38:39<2:16:34, 10.52s/it]\u001b[A\n 22%|██▏       | 223/1001 [38:49<2:16:04, 10.49s/it]\u001b[A\n 22%|██▏       | 224/1001 [39:00<2:15:42, 10.48s/it]\u001b[A\n 22%|██▏       | 225/1001 [39:10<2:15:25, 10.47s/it]\u001b[A\n 23%|██▎       | 226/1001 [39:21<2:15:07, 10.46s/it]\u001b[A\n 23%|██▎       | 227/1001 [39:31<2:14:51, 10.45s/it]\u001b[A\n 23%|██▎       | 228/1001 [39:41<2:14:36, 10.45s/it]\u001b[A\n 23%|██▎       | 229/1001 [39:52<2:14:23, 10.45s/it]\u001b[A\n 23%|██▎       | 230/1001 [40:02<2:14:12, 10.44s/it]\u001b[A\n 23%|██▎       | 231/1001 [40:13<2:15:28, 10.56s/it]\u001b[A\n 23%|██▎       | 232/1001 [40:24<2:14:49, 10.52s/it]\u001b[A\n 23%|██▎       | 233/1001 [40:34<2:14:25, 10.50s/it]\u001b[A\n 23%|██▎       | 234/1001 [40:44<2:13:57, 10.48s/it]\u001b[A\n 23%|██▎       | 235/1001 [40:55<2:13:33, 10.46s/it]\u001b[A\n 24%|██▎       | 236/1001 [41:05<2:13:16, 10.45s/it]\u001b[A\n 24%|██▎       | 237/1001 [41:16<2:13:04, 10.45s/it]\u001b[A\n 24%|██▍       | 238/1001 [41:26<2:12:48, 10.44s/it]\u001b[A\n 24%|██▍       | 239/1001 [41:37<2:12:33, 10.44s/it]\u001b[A\n 24%|██▍       | 240/1001 [41:47<2:12:16, 10.43s/it]\u001b[A\n 24%|██▍       | 241/1001 [41:58<2:13:23, 10.53s/it]\u001b[A\n 24%|██▍       | 242/1001 [42:08<2:12:46, 10.50s/it]\u001b[A\n 24%|██▍       | 243/1001 [42:19<2:12:15, 10.47s/it]\u001b[A\n 24%|██▍       | 244/1001 [42:29<2:11:53, 10.45s/it]\u001b[A\n 24%|██▍       | 245/1001 [42:39<2:11:33, 10.44s/it]\u001b[A\n 25%|██▍       | 246/1001 [42:50<2:11:16, 10.43s/it]\u001b[A\n 25%|██▍       | 247/1001 [43:00<2:10:53, 10.42s/it]\u001b[A\n 25%|██▍       | 248/1001 [43:11<2:10:33, 10.40s/it]\u001b[A\n 25%|██▍       | 249/1001 [43:21<2:10:20, 10.40s/it]\u001b[A\n 25%|██▍       | 250/1001 [43:31<2:10:10, 10.40s/it]\u001b[A\n 25%|██▌       | 251/1001 [43:42<2:09:59, 10.40s/it]\u001b[A\n 25%|██▌       | 252/1001 [43:53<2:11:07, 10.50s/it]\u001b[A\n 25%|██▌       | 253/1001 [44:03<2:10:35, 10.48s/it]\u001b[A\n 25%|██▌       | 254/1001 [44:13<2:10:10, 10.46s/it]\u001b[A\n 25%|██▌       | 255/1001 [44:24<2:09:50, 10.44s/it]\u001b[A\n 26%|██▌       | 256/1001 [44:34<2:09:31, 10.43s/it]\u001b[A\n 26%|██▌       | 257/1001 [44:45<2:09:16, 10.42s/it]\u001b[A\n 26%|██▌       | 258/1001 [44:55<2:09:02, 10.42s/it]\u001b[A\n 26%|██▌       | 259/1001 [45:05<2:08:50, 10.42s/it]\u001b[A\n 26%|██▌       | 260/1001 [45:16<2:08:45, 10.43s/it]\u001b[A\n 26%|██▌       | 261/1001 [45:27<2:09:49, 10.53s/it]\u001b[A\n 26%|██▌       | 262/1001 [45:37<2:09:07, 10.48s/it]\u001b[A\n 26%|██▋       | 263/1001 [45:47<2:08:32, 10.45s/it]\u001b[A\n 26%|██▋       | 264/1001 [45:58<2:08:12, 10.44s/it]\u001b[A\n 26%|██▋       | 265/1001 [46:08<2:07:55, 10.43s/it]\u001b[A\n 27%|██▋       | 266/1001 [46:19<2:07:31, 10.41s/it]\u001b[A\n 27%|██▋       | 267/1001 [46:29<2:07:20, 10.41s/it]\u001b[A\n 27%|██▋       | 268/1001 [46:39<2:07:18, 10.42s/it]\u001b[A\n 27%|██▋       | 269/1001 [46:50<2:07:07, 10.42s/it]\u001b[A\n 27%|██▋       | 270/1001 [47:00<2:06:50, 10.41s/it]\u001b[A\n 27%|██▋       | 271/1001 [47:11<2:07:51, 10.51s/it]\u001b[A\n 27%|██▋       | 272/1001 [47:21<2:07:25, 10.49s/it]\u001b[A\n 27%|██▋       | 273/1001 [47:32<2:07:04, 10.47s/it]\u001b[A\n 27%|██▋       | 274/1001 [47:42<2:06:45, 10.46s/it]\u001b[A\n 27%|██▋       | 275/1001 [47:53<2:06:29, 10.45s/it]\u001b[A\n 28%|██▊       | 276/1001 [48:03<2:06:12, 10.44s/it]\u001b[A\n 28%|██▊       | 277/1001 [48:14<2:05:58, 10.44s/it]\u001b[A\n 28%|██▊       | 278/1001 [48:24<2:05:47, 10.44s/it]\u001b[A\n 28%|██▊       | 279/1001 [48:34<2:05:36, 10.44s/it]\u001b[A\n 28%|██▊       | 280/1001 [48:45<2:05:27, 10.44s/it]\u001b[A\n 28%|██▊       | 281/1001 [48:56<2:06:36, 10.55s/it]\u001b[A\n 28%|██▊       | 282/1001 [49:06<2:06:02, 10.52s/it]\u001b[A\n 28%|██▊       | 283/1001 [49:17<2:05:37, 10.50s/it]\u001b[A\n 28%|██▊       | 284/1001 [49:27<2:05:12, 10.48s/it]\u001b[A\n 28%|██▊       | 285/1001 [49:37<2:04:51, 10.46s/it]\u001b[A\n 29%|██▊       | 286/1001 [49:48<2:04:33, 10.45s/it]\u001b[A\n 29%|██▊       | 287/1001 [49:58<2:04:18, 10.45s/it]\u001b[A\n 29%|██▉       | 288/1001 [50:09<2:04:07, 10.45s/it]\u001b[A\n 29%|██▉       | 289/1001 [50:19<2:03:54, 10.44s/it]\u001b[A\n 29%|██▉       | 290/1001 [50:30<2:04:56, 10.54s/it]\u001b[A","output_type":"stream"}],"execution_count":null},{"id":"44042c5a","cell_type":"markdown","source":"```js\n// 7749 vocab, 64 layer 1\nFinal loss: 0.43634021282196045\nTrain Accuracy: 0.8201 (61019/80000)\nTest Accuracy: 0.8158 (15540/20000)\n\n// 100 vocab, 104 layer 1, 20 epoch\nFinal loss: 0.48798397183418274\nTrain Accuracy: 0.7697 (62999/80000), random guess accuracy: 0.6713\nTest Accuracy: 0.7691 (16053/20000), random guess accuracy: 0.6823\nPrecision: 0.7812, Recall: 0.9190, F1: 0.8445\n\n// -> 100 batches\nFinal loss: 0.4811912477016449\nTrain Accuracy: 0.7857 (62716/80000), random guess accuracy: 0.6713\nTest Accuracy: 0.7719 (15957/20000), random guess accuracy: 0.6823\nPrecision: 0.7846, Recall: 0.9176, F1: 0.8459\n\n// -> 1000 batch SIZE\nFinal loss: 0.45420441031455994\nTrain Accuracy: 0.7831 (62935/80000), random guess accuracy: 0.6713\nTest Accuracy: 0.7722 (15991/20000), random guess accuracy: 0.6823\nPrecision: 0.7842, Recall: 0.9190, F1: 0.8463\n\n// -> 78 layer 1\nFinal loss: 0.4723219573497772\nTrain Accuracy: 0.7814 (62978/80000), random guess accuracy: 0.6713\nTest Accuracy: 0.7721 (16049/20000), random guess accuracy: 0.6823\nPrecision: 0.7831, Recall: 0.9211, F1: 0.8465\n\n\n// 75 layer 1, 5 layer 2\nFinal loss: 0.47183775901794434\nTrain Accuracy: 0.7830 (61135/80000), random guess accuracy: 0.6713\nTest Accuracy: 0.7702 (15633/20000), random guess accuracy: 0.6823\nPrecision: 0.7894, Recall: 0.9044, F1: 0.8430\n```","metadata":{}},{"id":"9c6b174d","cell_type":"code","source":"def param_to_word_mapping(model: nn.Module) -> dict[str, float]:\n    # m is something like nn.Sequential(), which is a module\n    params = list(model.parameters())\n\n    mapping = zip(bag_of_words.keys(), params[0].flatten().tolist())\n\n    return {k: v for k, v in sorted(mapping, key=lambda x: x[1], reverse=True)}\n\n\nparam_to_word_mapping(m)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-01T00:14:00.619Z"}},"outputs":[],"execution_count":null},{"id":"0c0f2860","cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b5cf48cc","cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3ac6508b","cell_type":"code","source":"tm = time.time()\ntorch.save(m, f\"/kaggle/working/model_{tm}.pth\")\njson.dump(bag_of_words, open(f\"/kaggle/working/bag_of_words_{tm}.json\", \"w\"))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-01T00:14:00.619Z"}},"outputs":[],"execution_count":null},{"id":"f83a7301","cell_type":"code","source":"# torch.serialization.add_safe_globals([nn.Sequential, nn.Linear, nn.Sigmoid, nn.ReLU])\n# m2 = torch.load(\"model.pth\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-01T00:14:00.619Z"}},"outputs":[],"execution_count":null},{"id":"2caaf890","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}