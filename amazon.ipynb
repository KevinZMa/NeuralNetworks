{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b81e93f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#! pip install -U \"datasets<4.0.0\" torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739706ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "mps = torch.device(\"mps\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\", \"raw_review_Software\", trust_remote_code=True\n",
    ")\n",
    "# TODO: download to volume\n",
    "\"\"\"\n",
    "format:\n",
    "{rating: int, title: str, text: str}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aebafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560a895",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(dataset[\"full\"][\"text\"])  # 480000\n",
    "n_samples\n",
    "\n",
    "# linear to size of dataset\n",
    "# linear to number of params\n",
    "# linear to number of batches (not size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2136c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = n_samples // 100\n",
    "X = dataset[\"full\"][\"text\"][:n_samples]\n",
    "# X_test = dataset[\"full\"][\"text\"][200000:225000]\n",
    "\n",
    "y = torch.tensor(dataset[\"full\"][\"rating\"][:n_samples]) >= 4\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b7c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def split_text(text: str) -> list[str]:\n",
    "    text = text.lower().strip()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    words = re.split(r\"\\s+\", text)\n",
    "    return words\n",
    "\n",
    "\n",
    "total_words = 0\n",
    "\n",
    "\n",
    "def add_to_bag(text: str) -> None:\n",
    "    global total_words\n",
    "    words = split_text(text)\n",
    "    total_words += len(words)\n",
    "    for word in words:\n",
    "        bag_of_words[word] = bag_of_words.get(word, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d077ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "ignored_words = [\n",
    "    # articles\n",
    "    \"a\", \"an\", \"the\",\n",
    "    # pronouns\n",
    "    \"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"their\",\"his\",\"her\",\"its\",\"our\",\"their\",\n",
    "    # prepositions\n",
    "    \"at\", \"in\", \"on\", \"by\", \"for\", \"with\", \"without\", \"to\", \"from\", \"of\", \"about\", \"under\", \"over\", \"through\", \"between\", \"among\", \"during\", \"before\", \"after\", \"above\", \"below\", \"up\", \"down\", \"out\", \"off\", \"into\", \"onto\", \"upon\", \"within\", \"across\", \"along\", \"around\", \"behind\", \"beside\", \"beyond\", \"inside\", \"outside\", \"toward\", \"towards\", \"underneath\", \"against\", \"beneath\", \"near\", \"next\", \"past\", \"since\", \"until\", \"via\",\n",
    "    # conjunctions\n",
    "    \"and\", \"or\", \"but\", \"so\", \"yet\", \"nor\", \"as\", \"if\", \"when\", \"while\", \"because\", \"although\", \"though\", \"unless\", \"whereas\", \"however\", \"therefore\", \"moreover\", \"furthermore\", \"nevertheless\", \"meanwhile\",\n",
    "    # common verbs\n",
    "    \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"do\", \"does\", \"did\", \"will\", \"would\", \"could\", \"should\", \"can\", \"may\", \"might\", \"must\", \"shall\", \"get\", \"got\", \"go\", \"went\", \"come\", \"came\", \"see\", \"saw\", \"know\", \"knew\", \"think\", \"thought\", \"say\", \"said\", \"tell\", \"told\", \"make\", \"made\", \"take\", \"took\", \"give\", \"gave\", \"find\", \"found\", \"use\", \"used\", \"work\", \"worked\", \"look\", \"looked\", \"seem\", \"seemed\", \"feel\", \"felt\", \"try\", \"tried\", \"leave\", \"left\", \"put\", \"set\", \"keep\", \"kept\", \"let\", \"run\", \"ran\", \"move\", \"moved\", \"live\", \"lived\", \"bring\", \"brought\", \"happen\", \"happened\", \"write\", \"wrote\", \"sit\", \"sat\", \"stand\", \"stood\", \"lose\", \"lost\", \"pay\", \"paid\", \"meet\", \"met\", \"include\", \"included\", \"continue\", \"continued\", \"turn\", \"turned\", \"follow\", \"followed\", \"want\", \"wanted\", \"need\", \"needed\", \"like\", \"liked\", \"help\", \"helped\", \"talk\", \"talked\", \"become\", \"became\", \"show\", \"showed\", \"hear\", \"heard\", \"play\", \"played\", \"run\", \"ran\", \"move\", \"moved\", \"live\", \"lived\", \"believe\", \"believed\", \"hold\", \"held\", \"bring\", \"brought\", \"happen\", \"happened\", \"write\", \"wrote\", \"provide\", \"provided\", \"sit\", \"sat\", \"stand\", \"stood\", \"lose\", \"lost\", \"pay\", \"paid\", \"meet\", \"met\", \"include\", \"included\",\n",
    "    # adverbs\n",
    "    \"very\", \"really\", \"quite\", \"just\", \"only\", \"also\", \"too\", \"so\", \"more\", \"most\", \"much\", \"many\", \"well\", \"good\", \"better\", \"best\", \"bad\", \"worse\", \"worst\", \"little\", \"less\", \"least\", \"big\", \"bigger\", \"biggest\", \"small\", \"smaller\", \"smallest\", \"long\", \"longer\", \"longest\", \"short\", \"shorter\", \"shortest\", \"high\", \"higher\", \"highest\", \"low\", \"lower\", \"lowest\", \"first\", \"last\", \"next\", \"previous\", \"new\", \"old\", \"young\", \"great\", \"right\", \"wrong\", \"true\", \"false\", \"sure\", \"probably\", \"maybe\", \"perhaps\", \"definitely\", \"certainly\", \"absolutely\", \"completely\", \"totally\", \"exactly\", \"almost\", \"nearly\", \"hardly\", \"barely\", \"quite\", \"rather\", \"pretty\", \"fairly\", \"somewhat\", \"slightly\", \"extremely\", \"incredibly\", \"amazingly\", \"surprisingly\", \"unfortunately\", \"fortunately\", \"obviously\", \"clearly\", \"apparently\", \"generally\", \"usually\", \"normally\", \"typically\", \"often\", \"sometimes\", \"rarely\", \"never\", \"always\", \"already\", \"still\", \"yet\", \"soon\", \"now\", \"then\", \"here\", \"there\", \"where\", \"everywhere\", \"anywhere\", \"somewhere\", \"nowhere\", \"how\", \"why\", \"what\", \"when\", \"who\", \"which\", \"whose\", \"whom\",\n",
    "    # numbers and quantities\n",
    "    \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"first\", \"second\", \"third\", \"another\", \"other\", \"others\", \"some\", \"any\", \"all\", \"each\", \"every\", \"both\", \"either\", \"neither\", \"none\", \"few\", \"several\", \"many\", \"most\", \"much\", \"little\", \"less\", \"more\", \"enough\", \"plenty\",\n",
    "    # misc common words\n",
    "    \"thing\", \"things\", \"something\", \"anything\", \"nothing\", \"everything\", \"someone\", \"anyone\", \"everyone\", \"no\", \"yes\", \"ok\", \"okay\", \"please\", \"thanks\", \"thank\", \"welcome\", \"hello\", \"hi\", \"bye\", \"goodbye\", \"sorry\", \"excuse\", \"pardon\", \"way\", \"ways\", \"time\", \"times\", \"day\", \"days\", \"year\", \"years\", \"place\", \"places\", \"people\", \"person\", \"man\", \"woman\", \"child\", \"children\", \"life\", \"world\", \"home\", \"house\", \"work\", \"job\", \"money\", \"business\", \"company\", \"part\", \"parts\", \"number\", \"numbers\", \"group\", \"groups\", \"problem\", \"problems\", \"question\", \"questions\", \"answer\", \"answers\", \"fact\", \"facts\", \"example\", \"examples\", \"case\", \"cases\", \"point\", \"points\", \"idea\", \"ideas\", \"information\", \"data\", \"result\", \"results\", \"change\", \"changes\", \"end\", \"beginning\", \"start\", \"finish\", \"side\", \"sides\", \"hand\", \"hands\", \"eye\", \"eyes\", \"head\", \"face\", \"back\", \"front\", \"top\", \"bottom\", \"left\", \"right\", \"inside\", \"outside\", \"important\", \"different\", \"same\", \"such\", \"even\", \"still\", \"however\", \"though\", \"although\", \"since\", \"while\", \"during\", \"before\", \"after\", \"until\", \"unless\", \"because\", \"if\", \"whether\", \"that\", \"this\", \"these\", \"those\", \"there\", \"here\", \"where\", \"when\", \"how\", \"why\", \"what\", \"who\", \"which\", \"whose\", \"whom\",\n",
    "    # common contractions\n",
    "    \"ive\", \"im\",\n",
    "    # parsing-specific\n",
    "    \"\", \"br\"\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "bag_of_words = {}\n",
    "\n",
    "for observation in X:\n",
    "    add_to_bag(observation)\n",
    "\n",
    "print(len(bag_of_words))\n",
    "\n",
    "\n",
    "bag_of_words = {\n",
    "    k: v for k, v in bag_of_words.items() if v > 10 and k not in ignored_words\n",
    "}\n",
    "print(len(bag_of_words))\n",
    "bag_size = 3000\n",
    "bag_of_words = dict(\n",
    "    sorted(bag_of_words.items(), key=lambda x: x[1], reverse=True)[:bag_size]\n",
    ")\n",
    "\n",
    "\n",
    "print(bag_of_words)\n",
    "print(len(bag_of_words))\n",
    "print(\"avg words per review:\", total_words / len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46450363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text: str, word_to_idx: dict[str, int]) -> torch.Tensor:\n",
    "    words = split_text(text)\n",
    "\n",
    "    word_counts = {}  # don't allocate all indices\n",
    "\n",
    "    for word in words:\n",
    "        idx = word_to_idx.get(word)\n",
    "        if idx is not None:\n",
    "            word_counts[idx] = word_counts.get(idx, 0) + 1\n",
    "\n",
    "    if not word_counts:\n",
    "        return torch.sparse_coo_tensor(\n",
    "            torch.empty(\n",
    "                (1, 0),  # 1 dimension, 0 non-zero elements (nnz)\n",
    "                dtype=torch.long\n",
    "            ),\n",
    "            [],\n",
    "            (len(word_to_idx),),\n",
    "            dtype=torch.float16,\n",
    "        )\n",
    "\n",
    "    indices = torch.tensor([list(word_counts.keys())], dtype=torch.long)\n",
    "    values = torch.tensor(list(word_counts.values()), dtype=torch.float16)\n",
    "\n",
    "    return torch.sparse_coo_tensor(\n",
    "        indices, values, (len(word_to_idx),), dtype=torch.float16\n",
    "    )\n",
    "\n",
    "\n",
    "def vectorize_parallel(\n",
    "    texts: list[str], word_to_idx: dict[str, int], n_jobs: int = 4\n",
    ") -> torch.Tensor:\n",
    "    vectors = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(vectorize)(text, word_to_idx) for text in texts\n",
    "    )\n",
    "    return torch.stack(vectors)#.to(mps)\n",
    "\n",
    "\n",
    "def preprocess(X: list[str]) -> torch.Tensor:\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(bag_of_words.keys())}\n",
    "\n",
    "    return vectorize_parallel(X, word_to_idx)\n",
    "\n",
    "\n",
    "X_processed = preprocess(X)\n",
    "# memory\n",
    "print(X_processed.element_size() * X_processed.numel() / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54abd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bag_of_words))\n",
    "print(len(X_processed))\n",
    "\n",
    "LAYERS = 124, 42, 14, 6\n",
    "# LAYER_1, LAYER_2 = 75, 5  # original\n",
    "LAYER_1, LAYER_2, LAYER_3, LAYER_4 = LAYERS\n",
    "\n",
    "bag_size = len(bag_of_words)\n",
    "\n",
    "print(\"num params L1:\", len(bag_of_words) * LAYER_1 + LAYER_1)\n",
    "num_params = (\n",
    "    sum(LAYERS[i - 1] * LAYERS[i] + LAYERS[i] for i in range(1, len(LAYERS)))\n",
    "    + bag_size * LAYERS[0]\n",
    "    + LAYERS[0]\n",
    "    + LAYERS[-1]\n",
    "    + 1\n",
    ")\n",
    "print(\"num params:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98b7f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, device=None, dtype=None):\n",
    "        \"\"\"Source copied from torch.nn.Linear\"\"\"\n",
    "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.empty((out_features, in_features), **factory_kwargs)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_sparse = x.coalesce()  # ensures unique indices\n",
    "        out = torch.sparse.mm(\n",
    "            x_sparse,  # (B x in) · (out x in)ᵀ -> (B x out)\n",
    "            self.weight.t(),\n",
    "        )\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec154c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"PARAMS\": num_params,\n",
    "    \"BATCHES\": 1000,\n",
    "    \"EPOCHS\": 10,\n",
    "    \"SAMPLES\": int(n_samples * 0.8),\n",
    "    \"LR\": 0.003,\n",
    "    \"BAG_SIZE\": bag_size,\n",
    "}\n",
    "if False:  # test runs\n",
    "    CONFIG = {\n",
    "        \"PARAMS\": num_params,\n",
    "        \"BATCHES\": 1000,\n",
    "        \"EPOCHS\": 10,\n",
    "        \"SAMPLES\": int(n_samples * 0.8),\n",
    "        \"LR\": 0.003,\n",
    "        \"BAG_SIZE\": bag_size,\n",
    "    }\n",
    "\n",
    "\n",
    "print(CONFIG)\n",
    "\n",
    "\n",
    "def run_nn():\n",
    "    # optimal params = 10% of 80000 = 8000\n",
    "    model = nn.Sequential(\n",
    "        SparseLinear(len(bag_of_words), LAYER_1),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(LAYER_1, LAYER_2),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(LAYER_2, LAYER_3),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(LAYER_3, LAYER_4),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(LAYER_4, 1),\n",
    "        # nn.Sigmoid(),  # using logits loss\n",
    "    )#.to(mps)\n",
    "    model = torch.compile(model)\n",
    "\n",
    "    # TODO: clear model if instantiating once\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"LR\"])\n",
    "\n",
    "    n_samples = CONFIG[\"SAMPLES\"]\n",
    "    X_train, X_test = X_processed[:n_samples], X_processed[n_samples:]\n",
    "    y_train, y_test = y[:n_samples].float(), y[n_samples:].float()\n",
    "\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=len(X_train) // CONFIG[\"BATCHES\"], shuffle=True\n",
    "    )\n",
    "    # TODO: sparse tensors?\n",
    "\n",
    "    # loss_per_batch = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(CONFIG[\"EPOCHS\"]):\n",
    "        print(f\"[{time.time()}] Epoch {epoch}\")\n",
    "        for X_batch, y_batch in tqdm(dataloader):\n",
    "            # already allocated, removing to(mps) shaved 33% of time\n",
    "            X_batch, y_batch = X_batch, y_batch\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred.squeeze(-1), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # loss_per_batch.append(loss.item())\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred_train = model(X_train).squeeze(-1)\n",
    "        y_pred_test = model(X_test).squeeze(-1)\n",
    "\n",
    "    # sigmoid to binary\n",
    "    train_preds = (y_pred_train >= 0.5).int()\n",
    "    test_preds = (y_pred_test >= 0.5).int()\n",
    "\n",
    "    num_true_train = (y_train == 1).sum()\n",
    "    num_true_test = (y_test == 1).sum()\n",
    "    train_acc = (train_preds == y_train.int()).float().mean()\n",
    "    test_acc = (test_preds == y_test.int()).float().mean()\n",
    "\n",
    "    # plt.xlabel(\"Batch\")\n",
    "    # plt.ylabel(\"Loss\")\n",
    "    # plt.plot(loss_per_batch)\n",
    "    # plt.show()\n",
    "    # print(\"Final loss:\", loss_per_batch[-1])\n",
    "\n",
    "    print(\n",
    "        f\"Train Accuracy: {train_acc:.4f} ({train_preds.sum()}/{len(X_train)}), random guess accuracy: {max(num_true_train, len(X_train) - num_true_train) / len(X_train):.4f}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Test Accuracy: {test_acc:.4f} ({test_preds.sum()}/{len(X_test)}), random guess accuracy: {max(num_true_test, len(X_test) - num_true_test) / len(X_test):.4f}\"\n",
    "    )\n",
    "\n",
    "    precision = (\n",
    "        test_preds * y_test\n",
    "    ).sum() / test_preds.sum()  # true pos / predicted pos\n",
    "    recall = (test_preds * y_test).sum() / y_test.sum()  # true pos / actual pos\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "m = run_nn()\n",
    "print(CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44042c5a",
   "metadata": {},
   "source": [
    "```js\n",
    "// 7749 vocab, 64 layer 1\n",
    "Final loss: 0.43634021282196045\n",
    "Train Accuracy: 0.8201 (61019/80000)\n",
    "Test Accuracy: 0.8158 (15540/20000)\n",
    "\n",
    "// 100 vocab, 104 layer 1, 20 epoch\n",
    "Final loss: 0.48798397183418274\n",
    "Train Accuracy: 0.7697 (62999/80000), random guess accuracy: 0.6713\n",
    "Test Accuracy: 0.7691 (16053/20000), random guess accuracy: 0.6823\n",
    "Precision: 0.7812, Recall: 0.9190, F1: 0.8445\n",
    "\n",
    "// -> 100 batches\n",
    "Final loss: 0.4811912477016449\n",
    "Train Accuracy: 0.7857 (62716/80000), random guess accuracy: 0.6713\n",
    "Test Accuracy: 0.7719 (15957/20000), random guess accuracy: 0.6823\n",
    "Precision: 0.7846, Recall: 0.9176, F1: 0.8459\n",
    "\n",
    "// -> 1000 batch SIZE\n",
    "Final loss: 0.45420441031455994\n",
    "Train Accuracy: 0.7831 (62935/80000), random guess accuracy: 0.6713\n",
    "Test Accuracy: 0.7722 (15991/20000), random guess accuracy: 0.6823\n",
    "Precision: 0.7842, Recall: 0.9190, F1: 0.8463\n",
    "\n",
    "// -> 78 layer 1\n",
    "Final loss: 0.4723219573497772\n",
    "Train Accuracy: 0.7814 (62978/80000), random guess accuracy: 0.6713\n",
    "Test Accuracy: 0.7721 (16049/20000), random guess accuracy: 0.6823\n",
    "Precision: 0.7831, Recall: 0.9211, F1: 0.8465\n",
    "\n",
    "\n",
    "// 75 layer 1, 5 layer 2\n",
    "Final loss: 0.47183775901794434\n",
    "Train Accuracy: 0.7830 (61135/80000), random guess accuracy: 0.6713\n",
    "Test Accuracy: 0.7702 (15633/20000), random guess accuracy: 0.6823\n",
    "Precision: 0.7894, Recall: 0.9044, F1: 0.8430\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_to_word_mapping(model: nn.Module) -> dict[str, float]:\n",
    "    # m is something like nn.Sequential(), which is a module\n",
    "    params = list(model.parameters())\n",
    "\n",
    "    mapping = zip(bag_of_words.keys(), params[0].flatten().tolist())\n",
    "\n",
    "    return {k: v for k, v in sorted(mapping, key=lambda x: x[1], reverse=True)}\n",
    "\n",
    "\n",
    "param_to_word_mapping(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cf48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = time.time()\n",
    "torch.save(m, f\"model_{tm}.pth\")\n",
    "json.dump(bag_of_words, open(f\"bag_of_words_{tm}.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83a7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.serialization.add_safe_globals([nn.Sequential, nn.Linear, nn.Sigmoid, nn.ReLU])\n",
    "# m2 = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caaf890",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
